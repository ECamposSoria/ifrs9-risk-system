"""
Performance benchmark tests for Polars integration in IFRS9 system.

Tests performance comparisons between Polars and pandas operations,
memory usage optimization, and ML pipeline efficiency.
"""

import pytest
import time
import numpy as np
import pandas as pd
import polars as pl
from datetime import datetime
import psutil
import os
import sys
from typing import Dict, Any

# Import the Polars integration module
sys.path.append('/app/src')
try:
    from polars_ml_integration import PolarsEnhancedCreditRiskClassifier, create_synthetic_ifrs9_data_polars
    POLARS_INTEGRATION_AVAILABLE = True
except ImportError as e:
    POLARS_INTEGRATION_AVAILABLE = False
    print(f"Polars integration not available: {e}")


@pytest.mark.skipif(not POLARS_INTEGRATION_AVAILABLE, reason="Polars integration not available")
class TestPolarsPerformanceBenchmarks:
    """Performance benchmark tests for Polars operations."""
    
    @pytest.fixture
    def small_dataset(self):
        """Small dataset for quick tests."""
        return create_synthetic_ifrs9_data_polars(10000)
    
    @pytest.fixture
    def medium_dataset(self):
        """Medium dataset for performance testing."""
        return create_synthetic_ifrs9_data_polars(100000)
    
    @pytest.fixture
    def large_dataset(self):
        """Large dataset for stress testing."""
        return create_synthetic_ifrs9_data_polars(500000)
    
    def measure_memory_usage(self, func, *args, **kwargs):
        """Measure memory usage of a function."""
        process = psutil.Process(os.getpid())
        mem_before = process.memory_info().rss / 1024 / 1024  # MB
        
        result = func(*args, **kwargs)
        
        mem_after = process.memory_info().rss / 1024 / 1024  # MB
        memory_used = mem_after - mem_before
        
        return result, memory_used
    
    def test_dataframe_creation_performance(self):
        """Test DataFrame creation performance: Polars vs pandas."""
        n_rows = 100000
        
        # Generate data once
        data = {
            'loan_id': [f'L{i:08d}' for i in range(n_rows)],
            'amount': np.random.lognormal(10, 1, n_rows),
            'rate': np.random.uniform(0.02, 0.12, n_rows),
            'score': np.random.normal(700, 100, n_rows),
            'date': [datetime(2024, 1, 1) for _ in range(n_rows)]
        }\n        \n        # Benchmark Polars DataFrame creation\n        start_time = time.time()\n        pl_df = pl.DataFrame(data)\n        polars_creation_time = time.time() - start_time\n        \n        # Benchmark pandas DataFrame creation\n        start_time = time.time()\n        pd_df = pd.DataFrame(data)\n        pandas_creation_time = time.time() - start_time\n        \n        print(f\"\\nDataFrame Creation Benchmark ({n_rows:,} rows):\")\n        print(f\"Polars: {polars_creation_time:.4f}s\")\n        print(f\"Pandas: {pandas_creation_time:.4f}s\")\n        print(f\"Speedup: {pandas_creation_time/polars_creation_time:.2f}x\")\n        \n        # Verify same shape\n        assert pl_df.shape == pd_df.shape\n        \n        # Both should complete in reasonable time\n        assert polars_creation_time < 5.0\n        assert pandas_creation_time < 5.0\n    \n    def test_feature_engineering_performance(self, medium_dataset):\n        \"\"\"Compare feature engineering performance between Polars and pandas.\"\"\"\n        classifier = PolarsEnhancedCreditRiskClassifier()\n        \n        # Convert to pandas for comparison\n        pd_dataset = medium_dataset.to_pandas()\n        \n        # Benchmark Polars feature engineering\n        start_time = time.time()\n        pl_features, pl_feature_names = classifier.prepare_features_polars(medium_dataset)\n        polars_time = time.time() - start_time\n        \n        # Benchmark equivalent pandas operations\n        start_time = time.time()\n        pd_features = pd_dataset.copy()\n        # Simplified pandas feature engineering for comparison\n        pd_features[\"debt_to_income\"] = pd_features[\"loan_amount\"] / pd_features[\"customer_income\"]\n        pd_features[\"payment_burden\"] = pd_features[\"monthly_payment\"] / pd_features[\"customer_income\"] * 12\n        pd_features[\"utilization_rate\"] = pd_features[\"current_balance\"] / pd_features[\"loan_amount\"]\n        pd_features[\"is_90_dpd\"] = (pd_features[\"days_past_due\"] > 90).astype(int)\n        pd_features[\"high_ltv\"] = (pd_features[\"ltv_ratio\"] > 0.8).astype(int)\n        # Add categorical dummies\n        loan_type_dummies = pd.get_dummies(pd_features[\"loan_type\"], prefix=\"loan_type\")\n        employment_dummies = pd.get_dummies(pd_features[\"employment_status\"], prefix=\"employment\")\n        pd_features_final = pd.concat([pd_features, loan_type_dummies, employment_dummies], axis=1)\n        pandas_time = time.time() - start_time\n        \n        speedup = pandas_time / polars_time if polars_time > 0 else 0\n        \n        print(f\"\\nFeature Engineering Benchmark ({len(medium_dataset):,} rows):\")\n        print(f\"Polars: {polars_time:.4f}s\")\n        print(f\"Pandas: {pandas_time:.4f}s\")\n        print(f\"Speedup: {speedup:.2f}x\")\n        print(f\"Polars features: {len(pl_feature_names)}\")\n        \n        # Verify results\n        assert len(pl_features) > 0\n        assert len(pl_feature_names) > 15  # Should have many features\n        \n        # Performance assertions\n        assert polars_time < 30.0  # Should complete within reasonable time\n        # Polars should generally be faster or comparable\n        # assert speedup >= 0.5  # Allow some variance in performance\n    \n    def test_aggregation_performance(self, medium_dataset):\n        \"\"\"Test aggregation operations performance.\"\"\"\n        pd_dataset = medium_dataset.to_pandas()\n        \n        # Polars aggregation\n        start_time = time.time()\n        pl_agg = (medium_dataset\n                 .group_by(['loan_type', 'employment_status'])\n                 .agg([\n                     pl.col('loan_amount').mean().alias('avg_amount'),\n                     pl.col('loan_amount').sum().alias('total_amount'),\n                     pl.col('credit_score').mean().alias('avg_score'),\n                     pl.col('days_past_due').max().alias('max_dpd'),\n                     pl.count().alias('count')\n                 ])\n                 .sort('total_amount', descending=True))\n        polars_agg_time = time.time() - start_time\n        \n        # Pandas aggregation\n        start_time = time.time()\n        pd_agg = (pd_dataset\n                 .groupby(['loan_type', 'employment_status'])\n                 .agg({\n                     'loan_amount': ['mean', 'sum'],\n                     'credit_score': 'mean',\n                     'days_past_due': 'max'\n                 })\n                 .reset_index())\n        pandas_agg_time = time.time() - start_time\n        \n        speedup = pandas_agg_time / polars_agg_time if polars_agg_time > 0 else 0\n        \n        print(f\"\\nAggregation Benchmark ({len(medium_dataset):,} rows):\")\n        print(f\"Polars: {polars_agg_time:.4f}s\")\n        print(f\"Pandas: {pandas_agg_time:.4f}s\")\n        print(f\"Speedup: {speedup:.2f}x\")\n        \n        # Verify results structure\n        assert len(pl_agg) > 0\n        assert len(pd_agg) > 0\n        \n        # Performance assertions\n        assert polars_agg_time < 10.0\n        assert pandas_agg_time < 10.0\n    \n    def test_filtering_performance(self, medium_dataset):\n        \"\"\"Test filtering operations performance.\"\"\"\n        pd_dataset = medium_dataset.to_pandas()\n        \n        # Complex filter conditions\n        filter_conditions = (\n            (pl.col('credit_score') > 650) &\n            (pl.col('ltv_ratio') < 0.9) &\n            (pl.col('days_past_due') < 30) &\n            (pl.col('loan_amount') > 50000)\n        )\n        \n        # Polars filtering\n        start_time = time.time()\n        pl_filtered = medium_dataset.filter(filter_conditions)\n        polars_filter_time = time.time() - start_time\n        \n        # Pandas filtering\n        start_time = time.time()\n        pd_filtered = pd_dataset[\n            (pd_dataset['credit_score'] > 650) &\n            (pd_dataset['ltv_ratio'] < 0.9) &\n            (pd_dataset['days_past_due'] < 30) &\n            (pd_dataset['loan_amount'] > 50000)\n        ]\n        pandas_filter_time = time.time() - start_time\n        \n        speedup = pandas_filter_time / polars_filter_time if polars_filter_time > 0 else 0\n        \n        print(f\"\\nFiltering Benchmark ({len(medium_dataset):,} rows):\")\n        print(f\"Polars: {polars_filter_time:.4f}s\")\n        print(f\"Pandas: {pandas_filter_time:.4f}s\")\n        print(f\"Speedup: {speedup:.2f}x\")\n        print(f\"Rows after filter - Polars: {len(pl_filtered)}, Pandas: {len(pd_filtered)}\")\n        \n        # Results should be similar\n        assert abs(len(pl_filtered) - len(pd_filtered)) <= 1  # Allow for small differences\n        \n        # Performance assertions\n        assert polars_filter_time < 5.0\n        assert pandas_filter_time < 5.0\n    \n    def test_memory_usage_comparison(self, medium_dataset):\n        \"\"\"Compare memory usage between Polars and pandas operations.\"\"\"\n        def polars_operations(df):\n            return (df\n                   .with_columns([\n                       (pl.col('loan_amount') / pl.col('customer_income')).alias('debt_ratio'),\n                       (pl.col('current_balance') / pl.col('loan_amount')).alias('util_ratio')\n                   ])\n                   .filter(pl.col('credit_score') > 600)\n                   .group_by('loan_type')\n                   .agg([pl.col('loan_amount').sum(), pl.count()])\n                   .sort('loan_amount_sum', descending=True))\n        \n        def pandas_operations(df):\n            df = df.copy()\n            df['debt_ratio'] = df['loan_amount'] / df['customer_income']\n            df['util_ratio'] = df['current_balance'] / df['loan_amount']\n            df = df[df['credit_score'] > 600]\n            return df.groupby('loan_type')['loan_amount'].agg(['sum', 'count']).sort_values('sum', ascending=False)\n        \n        # Measure Polars memory usage\n        pl_result, pl_memory = self.measure_memory_usage(polars_operations, medium_dataset)\n        \n        # Measure pandas memory usage\n        pd_dataset = medium_dataset.to_pandas()\n        pd_result, pd_memory = self.measure_memory_usage(pandas_operations, pd_dataset)\n        \n        print(f\"\\nMemory Usage Comparison ({len(medium_dataset):,} rows):\")\n        print(f\"Polars memory delta: {pl_memory:.2f} MB\")\n        print(f\"Pandas memory delta: {pd_memory:.2f} MB\")\n        \n        if pd_memory > 0:\n            memory_efficiency = pd_memory / pl_memory if pl_memory > 0 else float('inf')\n            print(f\"Polars memory efficiency: {memory_efficiency:.2f}x better\")\n        \n        # Both should produce results\n        assert len(pl_result) > 0\n        assert len(pd_result) > 0\n    \n    def test_lazy_evaluation_performance(self, large_dataset):\n        \"\"\"Test lazy evaluation performance benefits.\"\"\"\n        # Eager evaluation\n        start_time = time.time()\n        eager_result = (large_dataset\n                       .with_columns([\n                           (pl.col('loan_amount') / pl.col('customer_income')).alias('ratio1'),\n                           (pl.col('current_balance') / pl.col('loan_amount')).alias('ratio2')\n                       ])\n                       .filter(pl.col('credit_score') > 650)\n                       .group_by('loan_type')\n                       .agg([pl.col('loan_amount').sum(), pl.count()]))\n        eager_time = time.time() - start_time\n        \n        # Lazy evaluation\n        start_time = time.time()\n        lazy_result = (large_dataset.lazy()\n                      .with_columns([\n                          (pl.col('loan_amount') / pl.col('customer_income')).alias('ratio1'),\n                          (pl.col('current_balance') / pl.col('loan_amount')).alias('ratio2')\n                      ])\n                      .filter(pl.col('credit_score') > 650)\n                      .group_by('loan_type')\n                      .agg([pl.col('loan_amount').sum(), pl.count()])\n                      .collect())\n        lazy_time = time.time() - start_time\n        \n        efficiency = eager_time / lazy_time if lazy_time > 0 else 0\n        \n        print(f\"\\nLazy Evaluation Benchmark ({len(large_dataset):,} rows):\")\n        print(f\"Eager evaluation: {eager_time:.4f}s\")\n        print(f\"Lazy evaluation: {lazy_time:.4f}s\")\n        print(f\"Lazy efficiency: {efficiency:.2f}x\")\n        \n        # Results should be identical\n        assert eager_result.equals(lazy_result)\n        \n        # Performance assertions - lazy should be competitive or better\n        assert lazy_time < eager_time * 1.5  # Allow some variance\n    \n    @pytest.mark.slow\n    def test_ml_pipeline_performance(self, medium_dataset):\n        \"\"\"Test end-to-end ML pipeline performance.\"\"\"\n        classifier = PolarsEnhancedCreditRiskClassifier(model_type=\"sklearn\")\n        \n        # Full ML pipeline with Polars\n        start_time = time.time()\n        \n        # Feature engineering\n        features, feature_names = classifier.prepare_features_polars(medium_dataset)\n        target = medium_dataset['provision_stage']\n        \n        # Model training\n        metrics = classifier.train_stage_classifier_polars(features, target, test_size=0.2)\n        \n        total_pipeline_time = time.time() - start_time\n        \n        print(f\"\\nML Pipeline Performance ({len(medium_dataset):,} rows):\")\n        print(f\"Total pipeline time: {total_pipeline_time:.2f}s\")\n        print(f\"Model accuracy: {metrics['accuracy']:.4f}\")\n        print(f\"CV accuracy: {metrics['cv_mean']:.4f} ± {metrics['cv_std']:.4f}\")\n        \n        # Performance and quality assertions\n        assert total_pipeline_time < 120.0  # Should complete within 2 minutes\n        assert metrics['accuracy'] > 0.5  # Should have reasonable accuracy\n        assert len(feature_names) > 15  # Should create many features\n    \n    def test_data_type_optimization(self):\n        \"\"\"Test Polars data type optimization benefits.\"\"\"\n        n_rows = 100000\n        \n        # Create data with suboptimal types\n        data = {\n            'id': [f'ID{i:010d}' for i in range(n_rows)],  # String IDs\n            'category': ['A', 'B', 'C'] * (n_rows // 3 + 1),  # Repeating categories\n            'amount': np.random.uniform(1000, 100000, n_rows),  # Float64\n            'count': np.random.randint(1, 100, n_rows),  # Int64\n            'flag': np.random.choice([True, False], n_rows)  # Boolean\n        }\n        \n        # Create DataFrames\n        pd_df = pd.DataFrame(data)\n        pl_df = pl.DataFrame(data)\n        \n        # Optimize Polars dtypes\n        pl_df_optimized = pl_df.with_columns([\n            pl.col('category').cast(pl.Categorical),\n            pl.col('amount').cast(pl.Float32),\n            pl.col('count').cast(pl.UInt8),  # Assuming count < 256\n        ])\n        \n        # Memory usage comparison\n        pd_memory = pd_df.memory_usage(deep=True).sum() / 1024 / 1024  # MB\n        pl_memory = pl_df.estimated_size() / 1024 / 1024  # MB (approximation)\n        pl_optimized_memory = pl_df_optimized.estimated_size() / 1024 / 1024  # MB\n        \n        print(f\"\\nData Type Optimization ({n_rows:,} rows):\")\n        print(f\"Pandas memory: {pd_memory:.2f} MB\")\n        print(f\"Polars memory: {pl_memory:.2f} MB\")\n        print(f\"Polars optimized: {pl_optimized_memory:.2f} MB\")\n        \n        if pl_memory > 0:\n            optimization_factor = pl_memory / pl_optimized_memory\n            print(f\"Optimization factor: {optimization_factor:.2f}x\")\n            assert optimization_factor > 1.0  # Should reduce memory usage\n        \n        # Data integrity check\n        assert len(pl_df_optimized) == len(pd_df)\n        assert pl_df_optimized.shape == pd_df.shape\n\n\nif __name__ == \"__main__\":\n    # Run performance benchmarks when called directly\n    pytest.main([__file__, \"-v\", \"-s\"])