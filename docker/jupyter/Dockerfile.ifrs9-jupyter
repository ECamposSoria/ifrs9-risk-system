FROM jupyter/pyspark-notebook:python-3.10

USER root

# Install additional system dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    curl \
    && rm -rf /var/lib/apt/lists/*

USER ${NB_UID}

# Fix multimethod version conflict first
RUN pip uninstall -y multimethod || true

# Install essential packages including Polars, explicit PySpark installation and ML dependencies
RUN pip install --no-cache-dir \
    pyspark==3.4.0 \
    "polars[pandas,numpy,pyarrow]>=0.20.0,<1.0.0" \
    scikit-learn==1.3.2 \
    matplotlib==3.7.1 \
    seaborn==0.12.2 \
    plotly==5.15.0 \
    python-dotenv==1.0.0 \
    click==8.1.3 \
    PyYAML==6.0 \
    faker==18.13.0 \
    great-expectations==0.17.23 \
    multimethod==1.9.1 \
    pandera==0.17.2 \
    black==23.7.0 \
    flake8==6.0.0 \
    pytest==7.4.0 \
    pytest-cov==4.1.0 \
    pytest-xdist==3.3.1 \
    mypy==1.4.1 \
    xgboost>=1.7.0 \
    lightgbm>=3.3.0 \
    catboost>=1.2.0 \
    optuna>=3.0.0 \
    shap>=0.42.0 \
    google-cloud-core>=2.3.0 \
    google-cloud-storage>=2.8.0 \
    google-cloud-bigquery>=2.34.0

# Configure Spark and PySpark
ENV SPARK_HOME=/usr/local/spark
ENV PATH="${SPARK_HOME}/bin:${PATH}"
ENV PYTHONPATH="${SPARK_HOME}/python:${SPARK_HOME}/python/lib/py4j-*-src.zip:${PYTHONPATH}"
ENV PYSPARK_PYTHON=python3
ENV PYSPARK_DRIVER_PYTHON=jupyter
ENV PYSPARK_DRIVER_PYTHON_OPTS='lab'
ENV SPARK_MASTER=spark://spark-master:7077

# Set working directory
WORKDIR /home/jovyan

# Configure Jupyter
RUN jupyter lab build

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8888/api || exit 1