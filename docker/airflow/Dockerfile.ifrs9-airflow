FROM apache/airflow:2.7.0-python3.10

USER root

# Install Java for PySpark
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    openjdk-17-jre-headless \
    procps \
    && rm -rf /var/lib/apt/lists/*

# Set Java environment
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
ENV PATH="${JAVA_HOME}/bin:${PATH}"

# Install Spark
ENV SPARK_VERSION=3.4.1
ENV HADOOP_VERSION=3
ENV SPARK_HOME=/opt/spark
ENV PATH="${SPARK_HOME}/bin:${PATH}"
ENV PYTHONPATH="${SPARK_HOME}/python:${SPARK_HOME}/python/lib/py4j-0.10.9.7-src.zip:${PYTHONPATH}"

RUN curl -L https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz | \
    tar -xz -C /opt && \
    mv /opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} ${SPARK_HOME}

USER airflow

# Copy and install additional requirements in stages
COPY --chown=airflow:root requirements.txt /tmp/

# Install core dependencies first
RUN pip install --no-cache-dir --disable-pip-version-check --upgrade pip setuptools wheel

# Install compatible versions to avoid conflicts
RUN pip install --no-cache-dir \
    "pandas>=2.0.0,<2.1.0" \
    "numpy>=1.24.0,<1.25.0" \
    "pyarrow>=6.0.0,<7.0.0" \
    "pyspark>=3.4.0,<3.5.0"

# Fix multimethod version conflict first - uninstall any existing version
RUN pip uninstall -y multimethod || true

# Install compatible multimethod and pandera versions to avoid import errors
RUN pip install --no-cache-dir \
    multimethod==1.9.1 \
    pandera==0.17.2

# Install remaining requirements
RUN pip install --no-cache-dir -r /tmp/requirements.txt

# Configure Spark for Airflow
ENV PYSPARK_PYTHON=python3
ENV PYSPARK_DRIVER_PYTHON=python3